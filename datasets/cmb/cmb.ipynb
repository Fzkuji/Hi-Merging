{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T12:03:16.593550Z",
     "start_time": "2024-09-08T12:03:14.903206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载cmb数据集, json格式\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 读取json文件\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# 保存json文件\n",
    "def save_json_file(file_path, data):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f)\n",
    "    \n",
    "# 读取cmb数据集\n",
    "data_path = './CMB/CMB-Exam/CMB-train/CMB-train-merge.json'\n",
    "cmb_data = read_json_file(data_path)"
   ],
   "id": "c0542800b55838d1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T12:03:33.767665Z",
     "start_time": "2024-09-08T12:03:33.763511Z"
    }
   },
   "cell_type": "code",
   "source": "print(type(cmb_data))",
   "id": "e1c5f93d1e0c3b45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T12:03:51.420027Z",
     "start_time": "2024-09-08T12:03:51.209140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 转换成pandas的DataFrame\n",
    "cmb_df = pd.DataFrame(cmb_data)\n",
    "print(cmb_df.shape)"
   ],
   "id": "cf3881f504b01f7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(269359, 7)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T12:04:00.894707Z",
     "start_time": "2024-09-08T12:04:00.885736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 查看数据集的前几行\n",
    "cmb_df.head()"
   ],
   "id": "410981852c90f07e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  exam_type exam_class exam_subject             question answer question_type  \\\n",
       "0      医师考试       规培结业        临床病理科    HIV患者最常感染的是下列哪种肺炎      D         单项选择题   \n",
       "1      医师考试       规培结业          口腔科    下列部位的口腔黏膜上皮有角化，除了      D         单项选择题   \n",
       "2      医师考试       规培结业          皮肤科       细胞因子所不具备的作用特点是      B         单项选择题   \n",
       "3      医师考试       规培结业           骨科  按照三阶梯用药原则，适用于中度癌痛的是      E         单项选择题   \n",
       "4      医师考试       规培结业          妇产科    关于协调性宫缩乏力正确的是（ ）。      A         单项选择题   \n",
       "\n",
       "                                              option  \n",
       "0  {'A': '大叶性肺炎', 'B': '小叶性肺炎', 'C': '非典型肺炎', 'D'...  \n",
       "1  {'A': '唇红', 'B': '硬腭', 'C': '牙龈', 'D': '舌腹', '...  \n",
       "2  {'A': '拮抗性', 'B': '特异性', 'C': '多效性', 'D': '重叠性...  \n",
       "3  {'A': '可待因、丁丙诺啡、布洛芬', 'B': '氢吗啡酮、右丙氧芬、安度芬', 'C...  \n",
       "4  {'A': '宫缩极性，对称性正常，仅收缩力弱', 'B': '多数产妇觉持续腹痛，且产程延...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exam_type</th>\n",
       "      <th>exam_class</th>\n",
       "      <th>exam_subject</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_type</th>\n",
       "      <th>option</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>医师考试</td>\n",
       "      <td>规培结业</td>\n",
       "      <td>临床病理科</td>\n",
       "      <td>HIV患者最常感染的是下列哪种肺炎</td>\n",
       "      <td>D</td>\n",
       "      <td>单项选择题</td>\n",
       "      <td>{'A': '大叶性肺炎', 'B': '小叶性肺炎', 'C': '非典型肺炎', 'D'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>医师考试</td>\n",
       "      <td>规培结业</td>\n",
       "      <td>口腔科</td>\n",
       "      <td>下列部位的口腔黏膜上皮有角化，除了</td>\n",
       "      <td>D</td>\n",
       "      <td>单项选择题</td>\n",
       "      <td>{'A': '唇红', 'B': '硬腭', 'C': '牙龈', 'D': '舌腹', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>医师考试</td>\n",
       "      <td>规培结业</td>\n",
       "      <td>皮肤科</td>\n",
       "      <td>细胞因子所不具备的作用特点是</td>\n",
       "      <td>B</td>\n",
       "      <td>单项选择题</td>\n",
       "      <td>{'A': '拮抗性', 'B': '特异性', 'C': '多效性', 'D': '重叠性...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>医师考试</td>\n",
       "      <td>规培结业</td>\n",
       "      <td>骨科</td>\n",
       "      <td>按照三阶梯用药原则，适用于中度癌痛的是</td>\n",
       "      <td>E</td>\n",
       "      <td>单项选择题</td>\n",
       "      <td>{'A': '可待因、丁丙诺啡、布洛芬', 'B': '氢吗啡酮、右丙氧芬、安度芬', 'C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>医师考试</td>\n",
       "      <td>规培结业</td>\n",
       "      <td>妇产科</td>\n",
       "      <td>关于协调性宫缩乏力正确的是（ ）。</td>\n",
       "      <td>A</td>\n",
       "      <td>单项选择题</td>\n",
       "      <td>{'A': '宫缩极性，对称性正常，仅收缩力弱', 'B': '多数产妇觉持续腹痛，且产程延...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T12:04:14.493178Z",
     "start_time": "2024-09-08T12:04:14.365237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 查看是否有缺失值\n",
    "cmb_df.isnull().sum()"
   ],
   "id": "a78fd356fbdb8d89",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exam_type        0\n",
       "exam_class       0\n",
       "exam_subject     0\n",
       "question         0\n",
       "answer           0\n",
       "question_type    0\n",
       "option           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T12:13:44.709702Z",
     "start_time": "2024-09-08T12:13:40.010168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# 定义 JSON 文件路径\n",
    "data_files = {\n",
    "    'train': './CMB/CMB-Exam/CMB-train/CMB-train-merge.json',\n",
    "    'valid': './CMB/CMB-Exam/CMB-val/CMB-val-merge.json',\n",
    "    'test': './CMB/CMB-Exam/CMB-test/CMB-test-choice-question-merge.json'\n",
    "}\n",
    "\n",
    "# 通过 load_dataset 读取 JSON 文件\n",
    "dataset_dict = load_dataset('json', data_files=data_files)\n",
    "\n",
    "# 查看数据集内容\n",
    "print(dataset_dict)\n",
    "\n",
    "# 保存数据集到本地\n",
    "save_path = './saved_dataset'\n",
    "dataset_dict.save_to_disk(save_path)\n",
    "\n",
    "print(f\"Dataset has been saved to {save_path}\")\n"
   ],
   "id": "dd2347a2d706544c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76659f105337407ba4699e056579cc44"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating valid split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a28e6e884fd48e4862703a509d3bec6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationCastError",
     "evalue": "An error occurred while generating the dataset\n\nAll the data files must have the same columns, but at some point there are 1 new columns ({'explanation'})\n\nThis happened while the json dataset builder was generating data using\n\nC:/Users/fzkuj/PycharmProjects/llm-merging/datasets/cmb/./CMB/CMB-Exam/CMB-val/CMB-val-merge.json\n\nPlease either edit the data files to have matching columns, or separate them into different configurations (see docs at https://hf.co/docs/hub/datasets-manual-configuration#multiple-configurations)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mCastError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\envs\\memory\\Lib\\site-packages\\datasets\\builder.py:2011\u001B[0m, in \u001B[0;36mArrowBasedBuilder._prepare_split_single\u001B[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001B[0m\n\u001B[0;32m   2010\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 2011\u001B[0m     writer\u001B[38;5;241m.\u001B[39mwrite_table(table)\n\u001B[0;32m   2012\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m CastError \u001B[38;5;28;01mas\u001B[39;00m cast_error:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\memory\\Lib\\site-packages\\datasets\\arrow_writer.py:585\u001B[0m, in \u001B[0;36mArrowWriter.write_table\u001B[1;34m(self, pa_table, writer_batch_size)\u001B[0m\n\u001B[0;32m    584\u001B[0m pa_table \u001B[38;5;241m=\u001B[39m pa_table\u001B[38;5;241m.\u001B[39mcombine_chunks()\n\u001B[1;32m--> 585\u001B[0m pa_table \u001B[38;5;241m=\u001B[39m table_cast(pa_table, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_schema)\n\u001B[0;32m    586\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_local_files:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\memory\\Lib\\site-packages\\datasets\\table.py:2295\u001B[0m, in \u001B[0;36mtable_cast\u001B[1;34m(table, schema)\u001B[0m\n\u001B[0;32m   2294\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m table\u001B[38;5;241m.\u001B[39mschema \u001B[38;5;241m!=\u001B[39m schema:\n\u001B[1;32m-> 2295\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast_table_to_schema(table, schema)\n\u001B[0;32m   2296\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m table\u001B[38;5;241m.\u001B[39mschema\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;241m!=\u001B[39m schema\u001B[38;5;241m.\u001B[39mmetadata:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\memory\\Lib\\site-packages\\datasets\\table.py:2249\u001B[0m, in \u001B[0;36mcast_table_to_schema\u001B[1;34m(table, schema)\u001B[0m\n\u001B[0;32m   2248\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28msorted\u001B[39m(table\u001B[38;5;241m.\u001B[39mcolumn_names) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28msorted\u001B[39m(features):\n\u001B[1;32m-> 2249\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CastError(\n\u001B[0;32m   2250\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt cast\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mtable\u001B[38;5;241m.\u001B[39mschema\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mto\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfeatures\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mbecause column names don\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt match\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   2251\u001B[0m         table_column_names\u001B[38;5;241m=\u001B[39mtable\u001B[38;5;241m.\u001B[39mcolumn_names,\n\u001B[0;32m   2252\u001B[0m         requested_column_names\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlist\u001B[39m(features),\n\u001B[0;32m   2253\u001B[0m     )\n\u001B[0;32m   2254\u001B[0m arrays \u001B[38;5;241m=\u001B[39m [cast_array_to_feature(table[name], feature) \u001B[38;5;28;01mfor\u001B[39;00m name, feature \u001B[38;5;129;01min\u001B[39;00m features\u001B[38;5;241m.\u001B[39mitems()]\n",
      "\u001B[1;31mCastError\u001B[0m: Couldn't cast\nanswer: string\nquestion_type: string\nexam_type: string\nexam_class: string\noption: struct<A: string, B: string, C: string, D: string, E: string, F: string>\n  child 0, A: string\n  child 1, B: string\n  child 2, C: string\n  child 3, D: string\n  child 4, E: string\n  child 5, F: string\nexam_subject: string\nexplanation: string\nquestion: string\nto\n{'answer': Value(dtype='string', id=None), 'question_type': Value(dtype='string', id=None), 'exam_type': Value(dtype='string', id=None), 'exam_class': Value(dtype='string', id=None), 'option': {'A': Value(dtype='string', id=None), 'B': Value(dtype='string', id=None), 'C': Value(dtype='string', id=None), 'D': Value(dtype='string', id=None), 'E': Value(dtype='string', id=None), 'F': Value(dtype='string', id=None)}, 'exam_subject': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None)}\nbecause column names don't match",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mDatasetGenerationCastError\u001B[0m                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 11\u001B[0m\n\u001B[0;32m      4\u001B[0m data_files \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./CMB/CMB-Exam/CMB-train/CMB-train-merge.json\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalid\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./CMB/CMB-Exam/CMB-val/CMB-val-merge.json\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./CMB/CMB-Exam/CMB-test/CMB-test-choice-question-merge.json\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      8\u001B[0m }\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# 通过 load_dataset 读取 JSON 文件\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m dataset_dict \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m'\u001B[39m, data_files\u001B[38;5;241m=\u001B[39mdata_files)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# 查看数据集内容\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(dataset_dict)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\memory\\Lib\\site-packages\\datasets\\load.py:2614\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[0;32m   2611\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m builder_instance\u001B[38;5;241m.\u001B[39mas_streaming_dataset(split\u001B[38;5;241m=\u001B[39msplit)\n\u001B[0;32m   2613\u001B[0m \u001B[38;5;66;03m# Download and prepare data\u001B[39;00m\n\u001B[1;32m-> 2614\u001B[0m builder_instance\u001B[38;5;241m.\u001B[39mdownload_and_prepare(\n\u001B[0;32m   2615\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[0;32m   2616\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m   2617\u001B[0m     verification_mode\u001B[38;5;241m=\u001B[39mverification_mode,\n\u001B[0;32m   2618\u001B[0m     num_proc\u001B[38;5;241m=\u001B[39mnum_proc,\n\u001B[0;32m   2619\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m   2620\u001B[0m )\n\u001B[0;32m   2622\u001B[0m \u001B[38;5;66;03m# Build dataset for splits\u001B[39;00m\n\u001B[0;32m   2623\u001B[0m keep_in_memory \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2624\u001B[0m     keep_in_memory \u001B[38;5;28;01mif\u001B[39;00m keep_in_memory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m is_small_dataset(builder_instance\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size)\n\u001B[0;32m   2625\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\memory\\Lib\\site-packages\\datasets\\builder.py:1027\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001B[0m\n\u001B[0;32m   1025\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_proc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1026\u001B[0m         prepare_split_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_proc\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_proc\n\u001B[1;32m-> 1027\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_download_and_prepare(\n\u001B[0;32m   1028\u001B[0m         dl_manager\u001B[38;5;241m=\u001B[39mdl_manager,\n\u001B[0;32m   1029\u001B[0m         verification_mode\u001B[38;5;241m=\u001B[39mverification_mode,\n\u001B[0;32m   1030\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_split_kwargs,\n\u001B[0;32m   1031\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdownload_and_prepare_kwargs,\n\u001B[0;32m   1032\u001B[0m     )\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;66;03m# Sync info\u001B[39;00m\n\u001B[0;32m   1034\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(split\u001B[38;5;241m.\u001B[39mnum_bytes \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39msplits\u001B[38;5;241m.\u001B[39mvalues())\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\memory\\Lib\\site-packages\\datasets\\builder.py:1122\u001B[0m, in \u001B[0;36mDatasetBuilder._download_and_prepare\u001B[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001B[0m\n\u001B[0;32m   1118\u001B[0m split_dict\u001B[38;5;241m.\u001B[39madd(split_generator\u001B[38;5;241m.\u001B[39msplit_info)\n\u001B[0;32m   1120\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1121\u001B[0m     \u001B[38;5;66;03m# Prepare split will record examples associated to the split\u001B[39;00m\n\u001B[1;32m-> 1122\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_split(split_generator, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_split_kwargs)\n\u001B[0;32m   1123\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1124\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[0;32m   1125\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot find data file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1126\u001B[0m         \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_download_instructions \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1127\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mOriginal error:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1128\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(e)\n\u001B[0;32m   1129\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\memory\\Lib\\site-packages\\datasets\\builder.py:1882\u001B[0m, in \u001B[0;36mArrowBasedBuilder._prepare_split\u001B[1;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001B[0m\n\u001B[0;32m   1880\u001B[0m job_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m   1881\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m pbar:\n\u001B[1;32m-> 1882\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m job_id, done, content \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_split_single(\n\u001B[0;32m   1883\u001B[0m         gen_kwargs\u001B[38;5;241m=\u001B[39mgen_kwargs, job_id\u001B[38;5;241m=\u001B[39mjob_id, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_prepare_split_args\n\u001B[0;32m   1884\u001B[0m     ):\n\u001B[0;32m   1885\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[0;32m   1886\u001B[0m             result \u001B[38;5;241m=\u001B[39m content\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\memory\\Lib\\site-packages\\datasets\\builder.py:2013\u001B[0m, in \u001B[0;36mArrowBasedBuilder._prepare_split_single\u001B[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001B[0m\n\u001B[0;32m   2011\u001B[0m     writer\u001B[38;5;241m.\u001B[39mwrite_table(table)\n\u001B[0;32m   2012\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m CastError \u001B[38;5;28;01mas\u001B[39;00m cast_error:\n\u001B[1;32m-> 2013\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DatasetGenerationCastError\u001B[38;5;241m.\u001B[39mfrom_cast_error(\n\u001B[0;32m   2014\u001B[0m         cast_error\u001B[38;5;241m=\u001B[39mcast_error,\n\u001B[0;32m   2015\u001B[0m         builder_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mbuilder_name,\n\u001B[0;32m   2016\u001B[0m         gen_kwargs\u001B[38;5;241m=\u001B[39mgen_kwargs,\n\u001B[0;32m   2017\u001B[0m         token\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken,\n\u001B[0;32m   2018\u001B[0m     )\n\u001B[0;32m   2019\u001B[0m num_examples_progress_update \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(table)\n\u001B[0;32m   2020\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m>\u001B[39m _time \u001B[38;5;241m+\u001B[39m config\u001B[38;5;241m.\u001B[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "\u001B[1;31mDatasetGenerationCastError\u001B[0m: An error occurred while generating the dataset\n\nAll the data files must have the same columns, but at some point there are 1 new columns ({'explanation'})\n\nThis happened while the json dataset builder was generating data using\n\nC:/Users/fzkuj/PycharmProjects/llm-merging/datasets/cmb/./CMB/CMB-Exam/CMB-val/CMB-val-merge.json\n\nPlease either edit the data files to have matching columns, or separate them into different configurations (see docs at https://hf.co/docs/hub/datasets-manual-configuration#multiple-configurations)"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "342e0337be4117e9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
