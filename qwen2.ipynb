{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-03T04:10:01.630676Z",
     "start_time": "2024-09-03T04:09:21.715049Z"
    }
   },
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"mps\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-7B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")\n",
    "\n",
    "prompt = \"问题：胃癌最主要的转移途径是（　　）。\\n选项：\\n\\tA. 直接蔓延\\n\\tB. 淋巴转移\\n\\tC. 血行转移\\n\\tD. 盆腔转移\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"假设您是一名医生，请根据患者的症状回答以下选择题。\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4f6e2a840334aedb3f9ae38acc14bcb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't infer missing attention mask on `mps` device. Please provide an `attention_mask` or use a different device.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 23\u001B[0m\n\u001B[1;32m     16\u001B[0m text \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mapply_chat_template(\n\u001B[1;32m     17\u001B[0m     messages,\n\u001B[1;32m     18\u001B[0m     tokenize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     19\u001B[0m     add_generation_prompt\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     20\u001B[0m )\n\u001B[1;32m     21\u001B[0m model_inputs \u001B[38;5;241m=\u001B[39m tokenizer([text], return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 23\u001B[0m generated_ids \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[1;32m     24\u001B[0m     model_inputs\u001B[38;5;241m.\u001B[39minput_ids,\n\u001B[1;32m     25\u001B[0m     max_new_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m\n\u001B[1;32m     26\u001B[0m )\n\u001B[1;32m     27\u001B[0m generated_ids \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     28\u001B[0m     output_ids[\u001B[38;5;28mlen\u001B[39m(input_ids):] \u001B[38;5;28;01mfor\u001B[39;00m input_ids, output_ids \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(model_inputs\u001B[38;5;241m.\u001B[39minput_ids, generated_ids)\n\u001B[1;32m     29\u001B[0m ]\n\u001B[1;32m     31\u001B[0m response \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mbatch_decode(generated_ids, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m/opt/anaconda3/envs/memory/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/memory/lib/python3.11/site-packages/transformers/generation/utils.py:1727\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   1724\u001B[0m     model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m generation_config\u001B[38;5;241m.\u001B[39muse_cache\n\u001B[1;32m   1726\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwargs_has_attention_mask \u001B[38;5;129;01mand\u001B[39;00m requires_attention_mask \u001B[38;5;129;01mand\u001B[39;00m accepts_attention_mask:\n\u001B[0;32m-> 1727\u001B[0m     model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_attention_mask_for_generation(\n\u001B[1;32m   1728\u001B[0m         inputs_tensor, generation_config\u001B[38;5;241m.\u001B[39m_pad_token_tensor, generation_config\u001B[38;5;241m.\u001B[39m_eos_token_tensor\n\u001B[1;32m   1729\u001B[0m     )\n\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoder_outputs\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m model_kwargs:\n\u001B[1;32m   1732\u001B[0m     \u001B[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001B[39;00m\n\u001B[1;32m   1733\u001B[0m     model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_encoder_decoder_kwargs_for_generation(\n\u001B[1;32m   1734\u001B[0m         inputs_tensor, model_kwargs, model_input_name, generation_config\n\u001B[1;32m   1735\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/memory/lib/python3.11/site-packages/transformers/generation/utils.py:493\u001B[0m, in \u001B[0;36mGenerationMixin._prepare_attention_mask_for_generation\u001B[0;34m(self, inputs, pad_token_id, eos_token_id)\u001B[0m\n\u001B[1;32m    490\u001B[0m \u001B[38;5;66;03m# Otherwise we have may have information -> try to infer the attention mask\u001B[39;00m\n\u001B[1;32m    491\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmps\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    492\u001B[0m     \u001B[38;5;66;03m# mps does not support torch.isin (https://github.com/pytorch/pytorch/issues/77764)\u001B[39;00m\n\u001B[0;32m--> 493\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    494\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt infer missing attention mask on `mps` device. Please provide an `attention_mask` or use a different device.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    495\u001B[0m     )\n\u001B[1;32m    497\u001B[0m is_pad_token_in_inputs \u001B[38;5;241m=\u001B[39m (pad_token_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[1;32m    498\u001B[0m     torch\u001B[38;5;241m.\u001B[39misin(elements\u001B[38;5;241m=\u001B[39minputs, test_elements\u001B[38;5;241m=\u001B[39mpad_token_id)\u001B[38;5;241m.\u001B[39many()\n\u001B[1;32m    499\u001B[0m )\n\u001B[1;32m    500\u001B[0m is_pad_token_not_equal_to_eos_token_id \u001B[38;5;241m=\u001B[39m (eos_token_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m~\u001B[39m(\n\u001B[1;32m    501\u001B[0m     torch\u001B[38;5;241m.\u001B[39misin(elements\u001B[38;5;241m=\u001B[39meos_token_id, test_elements\u001B[38;5;241m=\u001B[39mpad_token_id)\u001B[38;5;241m.\u001B[39many()\n\u001B[1;32m    502\u001B[0m )\n",
      "\u001B[0;31mValueError\u001B[0m: Can't infer missing attention mask on `mps` device. Please provide an `attention_mask` or use a different device."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fc64561f4f72ba05"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
